---
phase: 08-pipeline-orchestration-public-api
plan: 02
type: tdd
wave: 2
depends_on: ["08-01"]
files_modified:
  - tests/test_pipeline_integration.py
  - src/semantic_model_generator/__init__.py
autonomous: true

must_haves:
  truths:
    - "End-to-end integration test passes with representative warehouse schema (dimensions + facts + relationships) using mocked database connection"
    - "Integration test verifies actual TMDL files are written to disk with correct structure"
    - "Public API is importable from the top-level package: from semantic_model_generator import generate_semantic_model, PipelineConfig"
    - "Multi-table integration test produces correct classification and relationship structure"
  artifacts:
    - path: "tests/test_pipeline_integration.py"
      provides: "End-to-end integration tests for folder output path"
      contains: "class TestEndToEndFolderOutput"
    - path: "src/semantic_model_generator/__init__.py"
      provides: "Top-level public API exports"
      contains: "generate_semantic_model"
  key_links:
    - from: "tests/test_pipeline_integration.py"
      to: "src/semantic_model_generator/pipeline.py"
      via: "imports PipelineConfig, generate_semantic_model"
      pattern: "from semantic_model_generator\\.pipeline import"
    - from: "src/semantic_model_generator/__init__.py"
      to: "src/semantic_model_generator/pipeline.py"
      via: "re-exports PipelineConfig, PipelineError, generate_semantic_model"
      pattern: "from semantic_model_generator\\.pipeline import"
---

<objective>
Create end-to-end integration tests proving the full pipeline works with representative warehouse schemas, and export the public API from the top-level package.

Purpose: Integration tests verify that all phases (3-7) work together correctly when orchestrated by generate_semantic_model(). These tests use mocked database connections but exercise the real filtering, classification, relationship inference, TMDL generation, and folder writing code paths. The public API exports make the library usable with a simple `from semantic_model_generator import generate_semantic_model, PipelineConfig`.

Output: Integration test file with representative test data, updated __init__.py with public API exports.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-pipeline-orchestration-public-api/08-RESEARCH.md
@.planning/phases/08-pipeline-orchestration-public-api/08-01-SUMMARY.md

@src/semantic_model_generator/pipeline.py
@src/semantic_model_generator/__init__.py
@src/semantic_model_generator/domain/types.py
@src/semantic_model_generator/schema/connection.py
@src/semantic_model_generator/schema/discovery.py
@src/semantic_model_generator/tmdl/generate.py
@src/semantic_model_generator/output/writer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED - Write failing integration tests and public API exports</name>
  <files>tests/test_pipeline_integration.py, src/semantic_model_generator/__init__.py</files>
  <action>
**Step 1: Create integration test file** `tests/test_pipeline_integration.py`

The integration tests mock ONLY the database connection layer (`mssql_python`). Everything else (filtering, classification, relationship inference, TMDL generation, folder writing) runs for real against tmp_path.

Define test data helper at module level to build mock cursor results. The mock cursor fetchall returns rows matching INFORMATION_SCHEMA.COLUMNS format:
`(TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE, IS_NULLABLE, ORDINAL_POSITION, CHARACTER_MAXIMUM_LENGTH, NUMERIC_PRECISION, NUMERIC_SCALE)`

**Test Data - Star Schema:**
A representative warehouse with 2 dimensions and 1 fact table:
```python
STAR_SCHEMA_ROWS = [
    # DimCustomer (1 key = dimension)
    ("dbo", "DimCustomer", "SK_Customer", "bigint", "NO", 1, None, 19, 0),
    ("dbo", "DimCustomer", "CustomerName", "varchar", "YES", 2, 100, None, None),
    ("dbo", "DimCustomer", "Email", "varchar", "YES", 3, 200, None, None),
    # DimProduct (1 key = dimension)
    ("dbo", "DimProduct", "SK_Product", "bigint", "NO", 1, None, 19, 0),
    ("dbo", "DimProduct", "ProductName", "varchar", "YES", 2, 150, None, None),
    ("dbo", "DimProduct", "Category", "varchar", "YES", 3, 50, None, None),
    # FactSales (3 keys = fact, references both dims + role-playing customer)
    ("dbo", "FactSales", "SK_Customer", "bigint", "NO", 1, None, 19, 0),
    ("dbo", "FactSales", "SK_Customer_ShipTo", "bigint", "NO", 2, None, 19, 0),
    ("dbo", "FactSales", "SK_Product", "bigint", "NO", 3, None, 19, 0),
    ("dbo", "FactSales", "Amount", "decimal", "NO", 4, None, 18, 2),
    ("dbo", "FactSales", "Quantity", "int", "NO", 5, None, 10, 0),
]
```

This data tests: dimension classification (1 key), fact classification (3 keys), direct relationship matching (SK_Customer, SK_Product), and role-playing dimension (SK_Customer_ShipTo -> DimCustomer with underscore boundary).

Create a pytest fixture `mock_connection` that patches `semantic_model_generator.schema.connection.mssql_python` and returns a mock connection whose cursor().fetchall() returns the test data.

**TestEndToEndFolderOutput** class (~7 tests):

- `test_full_pipeline_creates_tmdl_folder` - Call generate_semantic_model with folder config pointing at tmp_path. Assert result["mode"] == "folder" and result["output_path"] exists. Assert key files exist: `.platform`, `definition.pbism`, `definition/database.tmdl`, `definition/model.tmdl`, `definition/expressions.tmdl`, `definition/relationships.tmdl`, `definition/tables/DimCustomer.tmdl`, `definition/tables/DimProduct.tmdl`, `definition/tables/FactSales.tmdl`, `diagramLayout.json`.

- `test_dimension_classification_correct` - Read generated tables and verify DimCustomer.tmdl and DimProduct.tmdl contain proper dimension table TMDL content (check for `table DimCustomer` or `table DimProduct` in content).

- `test_fact_classification_correct` - Read generated FactSales.tmdl and verify it contains `table FactSales` content.

- `test_relationships_generated` - Read `definition/relationships.tmdl` from disk and verify it contains relationships. Check for `SK_Customer` and `SK_Product` relationship references. Verify role-playing: should have both active and inactive relationships to DimCustomer.

- `test_role_playing_dimension_detected` - Read relationships.tmdl. It should contain `isActive: false` for the second Customer relationship (SK_Customer_ShipTo is alphabetically after SK_Customer, so SK_Customer is active and SK_Customer_ShipTo is inactive).

- `test_dev_mode_creates_timestamped_folder` - Verify the output folder name contains an underscore followed by a timestamp pattern (the model name with timestamp appended).

- `test_write_summary_reports_all_files` - Check result["summary"] has non-empty written tuple containing all expected file paths.

**TestEndToEndWithFiltering** class (~2 tests):

- `test_include_filter_limits_tables` - Use include_tables=("DimCustomer", "FactSales") to exclude DimProduct. Verify output has DimCustomer.tmdl and FactSales.tmdl but NOT DimProduct.tmdl.

- `test_exclude_filter_removes_tables` - Use exclude_tables=("DimProduct",). Verify DimProduct.tmdl not in output.

**TestEndToEndErrorPaths** class (~2 tests):

- `test_connection_failure_gives_pipeline_error` - Mock connection to raise Exception. Verify PipelineError with stage="connection".

- `test_empty_schema_result_handled` - Mock cursor to return empty result. Verify pipeline completes (no tables = no files written) or produces appropriate result.

**TestPublicApiImports** class (~3 tests):

- `test_import_generate_semantic_model` - `from semantic_model_generator import generate_semantic_model` works
- `test_import_pipeline_config` - `from semantic_model_generator import PipelineConfig` works
- `test_import_pipeline_error` - `from semantic_model_generator import PipelineError` works

All integration tests should fail because imports from `__init__.py` will fail (exports not added yet) and the integration tests exercise real code paths that require proper orchestration from plan 08-01.

**Step 2: Update `src/semantic_model_generator/__init__.py`** to add public API exports:

Add imports and update `__all__`:
```python
from semantic_model_generator.pipeline import (
    PipelineConfig,
    PipelineError,
    generate_semantic_model,
)
```

Add to `__all__`: `"PipelineConfig"`, `"PipelineError"`, `"generate_semantic_model"`

Commit message: `test(08-02): add integration tests for end-to-end pipeline and public API exports`
  </action>
  <verify>
Run `python -m pytest tests/test_pipeline_integration.py -v` - all new tests MUST fail (either NotImplementedError from plan 01 stubs or test assertions). Run `python -m pytest --ignore=tests/test_pipeline_integration.py -q` - all existing tests MUST still pass. Run `make lint` and `make typecheck` - both MUST pass. Verify `python -c "from semantic_model_generator import generate_semantic_model, PipelineConfig, PipelineError"` works.
  </verify>
  <done>
All integration tests fail with expected errors. All existing tests pass. Public API exports work from top-level package. Lint and typecheck clean.
  </done>
</task>

<task type="auto">
  <name>Task 2: GREEN - Make integration tests pass</name>
  <files>tests/test_pipeline_integration.py</files>
  <action>
Since the pipeline module implementation was done in plan 08-01, the integration tests should largely pass once that plan is complete. This task focuses on fixing any test data issues or adjusting test assertions to match the actual pipeline behavior.

Specific areas that may need adjustment:

1. **Mock setup correctness** - Ensure the mssql_python mock is patched at the right module path (`semantic_model_generator.schema.connection.mssql_python`). The connection module imports mssql_python at the top level, so the patch must target that location.

2. **Cursor mock behavior** - The discover_tables function calls `conn.cursor()`, then `cursor.execute(query, list(schemas))`, then `cursor.fetchall()`. Ensure the mock chain is correct: `mock_mssql.connect.return_value` -> mock connection with `cursor()` returning a mock cursor with `execute()` and `fetchall()` methods.

3. **File path verification** - Check actual output folder structure against expected paths. The writer creates the folder structure `{model_name}_{timestamp}/` in dev mode. Integration tests should account for the dynamic timestamp in dev mode folder names.

4. **Relationship TMDL content** - The relationships.tmdl content uses specific TMDL syntax. Verify test assertions match the actual generated format (single-quoted table names, tab indentation).

5. **Write summary structure** - Verify WriteSummary attributes match what the tests expect: `written`, `skipped`, `unchanged` tuples and `output_path` Path.

6. **Empty schema handling** - If cursor returns empty rows, discover_tables returns empty tuple. filter_tables([]) returns []. classify_tables([]) returns {}. infer_relationships([], {}, prefixes) returns (). generate_all_tmdl with no tables should still produce base files (database.tmdl, model.tmdl, etc.) but no table files. Adjust test assertion accordingly.

Run tests iteratively, fixing any mismatches between test expectations and actual behavior. Do NOT modify the pipeline module implementation - only adjust test data or test assertions if they don't match the actual (correct) behavior.

Commit message: `feat(08-02): integration tests pass for end-to-end pipeline with public API exports`
  </action>
  <verify>
Run `python -m pytest tests/test_pipeline_integration.py -v` - all integration tests MUST pass. Run `python -m pytest -q` - ALL tests (354 existing + plan 01 tests + integration tests) MUST pass. Run `make check` - lint + typecheck + test all clean. Verify `python -c "from semantic_model_generator import generate_semantic_model, PipelineConfig, PipelineError; print('OK')"` prints OK.
  </verify>
  <done>
All integration tests pass. All existing tests pass. Public API importable from top-level package. Full pipeline orchestration verified end-to-end with representative star schema data including dimension/fact classification, relationship inference, role-playing detection, TMDL generation, and folder output. make check clean.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_pipeline_integration.py -v` - all integration tests pass
2. `python -m pytest -q` - all tests pass (354 + plan 01 + integration)
3. `make check` - lint + typecheck + test all green
4. `python -c "from semantic_model_generator import generate_semantic_model, PipelineConfig, PipelineError"` - public API importable
5. Integration test output folder contains correct TMDL structure with all expected files
6. Role-playing dimension detected with correct active/inactive marking in relationships.tmdl
7. Filtering (include/exclude) works end-to-end
</verification>

<success_criteria>
- End-to-end integration tests pass with representative star schema (2 dimensions, 1 fact, role-playing)
- Integration tests verify actual TMDL files written to disk via tmp_path
- Table classification (dimension vs fact) is correct in generated output
- Relationship inference including role-playing dimensions works end-to-end
- Include/exclude table filtering works end-to-end
- Error paths tested (connection failure produces PipelineError)
- Public API exports: from semantic_model_generator import generate_semantic_model, PipelineConfig, PipelineError
- All tests pass, make check clean
</success_criteria>

<output>
After completion, create `.planning/phases/08-pipeline-orchestration-public-api/08-02-SUMMARY.md`
</output>
