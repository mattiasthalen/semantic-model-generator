---
phase: 03-schema-discovery-classification
plan: 02
type: tdd
wave: 2
depends_on: ["03-01"]
files_modified:
  - pyproject.toml
  - src/semantic_model_generator/schema/connection.py
  - src/semantic_model_generator/schema/discovery.py
  - src/semantic_model_generator/schema/__init__.py
  - tests/schema/test_connection.py
  - tests/schema/test_discovery.py
autonomous: true

must_haves:
  truths:
    - "Library connects to a Fabric warehouse using pyodbc with token authentication"
    - "Token is encoded as UTF-16-LE and passed via SQL_COPT_SS_ACCESS_TOKEN (1256)"
    - "Connection string excludes Authentication, UID, PWD parameters when using token auth"
    - "Schema reader retrieves columns from INFORMATION_SCHEMA.COLUMNS for user-specified schemas only"
    - "Views are excluded from discovery by filtering TABLE_TYPE = 'BASE TABLE'"
    - "Discovered metadata is returned as tuple of TableMetadata frozen dataclasses"
    - "Schema discovery query joins INFORMATION_SCHEMA.TABLES with COLUMNS"
    - "Transient pyodbc errors are retried with exponential backoff via tenacity"
  artifacts:
    - path: "src/semantic_model_generator/schema/connection.py"
      provides: "Connection factory with token auth for Fabric warehouse"
      exports: ["create_fabric_connection", "encode_token_for_odbc"]
    - path: "src/semantic_model_generator/schema/discovery.py"
      provides: "INFORMATION_SCHEMA reader that builds TableMetadata from query results"
      exports: ["discover_tables"]
    - path: "pyproject.toml"
      provides: "New dependencies: pyodbc, azure-identity, tenacity"
      contains: "pyodbc"
    - path: "tests/schema/test_connection.py"
      provides: "Tests for token encoding, connection string building (mocked pyodbc)"
    - path: "tests/schema/test_discovery.py"
      provides: "Tests for schema discovery with mocked database cursor"
  key_links:
    - from: "src/semantic_model_generator/schema/discovery.py"
      to: "src/semantic_model_generator/domain/types.py"
      via: "imports ColumnMetadata, TableMetadata to build from query rows"
      pattern: "from semantic_model_generator\\.domain\\.types import"
    - from: "src/semantic_model_generator/schema/connection.py"
      to: "pyodbc"
      via: "pyodbc.connect with attrs_before for token"
      pattern: "pyodbc\\.connect"
    - from: "src/semantic_model_generator/schema/discovery.py"
      to: "src/semantic_model_generator/schema/connection.py"
      via: "accepts pyodbc.Connection as parameter (no direct import needed, dependency injection)"
      pattern: "conn.*Connection"
---

<objective>
Implement the Fabric warehouse connection factory (pyodbc + azure-identity token auth) and INFORMATION_SCHEMA discovery function that reads table/column metadata and builds immutable TableMetadata objects. This is the I/O boundary of the schema discovery pipeline.

Purpose: Connection and discovery are the data ingestion layer -- they bridge the external warehouse to the internal domain model. Without them, filtering and classification (plan 03-01) have no data to operate on. Token auth must follow the exact pyodbc pattern or connections fail silently.
Output: Tested connection.py and discovery.py modules. pyodbc, azure-identity, tenacity added as dependencies.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-schema-discovery-classification/03-RESEARCH.md
@.planning/phases/03-schema-discovery-classification/03-01-SUMMARY.md
@src/semantic_model_generator/domain/types.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD connection factory -- token encoding and pyodbc connection</name>
  <files>
    pyproject.toml
    src/semantic_model_generator/schema/connection.py
    src/semantic_model_generator/schema/__init__.py
    tests/schema/test_connection.py
  </files>
  <action>
First, add runtime dependencies to pyproject.toml:
```toml
dependencies = [
    "pyodbc>=5.0",
    "azure-identity>=1.19",
    "tenacity>=9.0",
]
```

Install them: `pip install -e .`

Also add mypy overrides for pyodbc and azure.identity (pyodbc may lack stubs):
```toml
[[tool.mypy.overrides]]
module = ["pyodbc", "azure.*"]
ignore_missing_imports = true
```

Note: The existing mypy overrides for "notebookutils.*" and "mssql.*" should remain. Add pyodbc and azure.* to a new override block.

RED phase -- Write tests/schema/test_connection.py FIRST:

Use unittest.mock to mock pyodbc.connect and azure.identity.DefaultAzureCredential. These tests verify the connection logic WITHOUT requiring an actual database.

1. Token encoding tests (encode_token_for_odbc function):
   Function signature: `encode_token_for_odbc(token: str) -> bytes`
   - Encodes a simple token string into UTF-16-LE bytes with length prefix
   - Output starts with 4-byte little-endian integer (struct.pack("<i", len))
   - The encoded bytes are UTF-16-LE (each ASCII byte followed by 0x00)
   - Empty token produces length-prefixed empty bytes (struct.pack("<i", 0))

2. Connection factory tests (create_fabric_connection):
   Function signature: `create_fabric_connection(sql_endpoint: str, database: str) -> pyodbc.Connection`
   - Calls DefaultAzureCredential().get_token with correct scope "https://database.windows.net//.default"
   - Passes token via attrs_before dict with key 1256 (SQL_COPT_SS_ACCESS_TOKEN)
   - Connection string contains "ODBC Driver 18 for SQL Server"
   - Connection string contains the provided sql_endpoint and database
   - Connection string contains "Encrypt=Yes" and "TrustServerCertificate=No"
   - Connection string does NOT contain "Authentication=", "UID=", or "PWD="
   - Calls pyodbc.connect with the connection string and attrs_before

3. Retry behavior test:
   - When pyodbc.connect raises pyodbc.OperationalError, retries up to 3 times
   - Uses mock side_effect to simulate transient failure then success

Mock strategy: Use @patch("semantic_model_generator.schema.connection.pyodbc") and @patch("semantic_model_generator.schema.connection.DefaultAzureCredential") to replace external dependencies.

Run tests -- they MUST fail.
Commit: "test(03-02): add failing tests for connection factory"

GREEN phase -- Create src/semantic_model_generator/schema/connection.py:

```python
"""Fabric warehouse connection factory with token authentication.

Connects to Microsoft Fabric warehouses using pyodbc + azure-identity.
Token auth uses UTF-16-LE encoding via SQL_COPT_SS_ACCESS_TOKEN (1256).

References:
- https://debruyn.dev/2023/connect-to-fabric-lakehouses-warehouses-from-python-code/
- https://learn.microsoft.com/en-us/fabric/data-warehouse/connectivity
"""

import struct
from itertools import chain, repeat

import pyodbc
from azure.identity import DefaultAzureCredential
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

# SQL_COPT_SS_ACCESS_TOKEN attribute code for pyodbc
_SQL_COPT_SS_ACCESS_TOKEN = 1256

# Azure SQL Database token scope
_TOKEN_SCOPE = "https://database.windows.net//.default"


def encode_token_for_odbc(token: str) -> bytes:
    """Encode authentication token as UTF-16-LE bytes with length prefix.

    pyodbc requires tokens encoded as:
    1. UTF-16-LE byte string (each byte interleaved with 0x00)
    2. Prefixed with 4-byte little-endian integer of encoded length
    """
    token_as_bytes = token.encode("UTF-8")
    encoded_bytes = bytes(chain.from_iterable(zip(token_as_bytes, repeat(0))))
    return struct.pack("<i", len(encoded_bytes)) + encoded_bytes


@retry(
    retry=retry_if_exception_type((pyodbc.OperationalError, pyodbc.InterfaceError)),
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
)
def create_fabric_connection(
    sql_endpoint: str,
    database: str,
) -> pyodbc.Connection:
    """Create authenticated connection to a Fabric warehouse.

    Uses DefaultAzureCredential for token acquisition (supports managed identity,
    CLI credentials, environment variables, etc.).

    Args:
        sql_endpoint: Fabric SQL analytics endpoint (e.g., xxx.datawarehouse.fabric.microsoft.com)
        database: Database name in the warehouse

    Returns:
        Authenticated pyodbc Connection

    Raises:
        pyodbc.OperationalError: After 3 retry attempts on transient failures
        azure.core.exceptions.ClientAuthenticationError: If no valid credential found
    """
    credential = DefaultAzureCredential()
    token_object = credential.get_token(_TOKEN_SCOPE)
    token_bytes = encode_token_for_odbc(token_object.token)

    attrs_before = {_SQL_COPT_SS_ACCESS_TOKEN: token_bytes}

    connection_string = (
        "Driver={ODBC Driver 18 for SQL Server};"
        f"Server={sql_endpoint},1433;"
        f"Database={database};"
        "Encrypt=Yes;"
        "TrustServerCertificate=No"
    )

    return pyodbc.connect(connection_string, attrs_before=attrs_before)
```

Update schema/__init__.py to re-export create_fabric_connection, encode_token_for_odbc.

Run tests -- they MUST pass. Run `make check` to confirm all green.
Commit: "feat(03-02): implement connection factory with token auth"
  </action>
  <verify>
    1. `make test` passes with all connection tests green
    2. `make check` passes (lint + typecheck + test)
    3. `python -c "from semantic_model_generator.schema.connection import create_fabric_connection, encode_token_for_odbc; print('OK')"` succeeds
    4. `pip install -e .` succeeds with pyodbc, azure-identity, tenacity resolved
  </verify>
  <done>
    Connection factory creates pyodbc connections with token auth via azure-identity DefaultAzureCredential. Token encoding produces correct UTF-16-LE bytes with length prefix. Connection string uses ODBC Driver 18, excludes auth parameters. Retry logic handles transient pyodbc errors with 3 attempts and exponential backoff. All tests pass using mocked pyodbc and azure-identity.
  </done>
</task>

<task type="auto">
  <name>Task 2: TDD schema discovery -- INFORMATION_SCHEMA reader with view filtering</name>
  <files>
    src/semantic_model_generator/schema/discovery.py
    src/semantic_model_generator/schema/__init__.py
    tests/schema/test_discovery.py
  </files>
  <action>
RED phase -- Write tests/schema/test_discovery.py FIRST:

Use unittest.mock to mock pyodbc.Connection and Cursor. Build mock cursor results that simulate INFORMATION_SCHEMA query output.

The discover_tables function signature:
```python
def discover_tables(
    conn: pyodbc.Connection,
    schemas: Sequence[str],
) -> tuple[TableMetadata, ...]:
```

1. Basic discovery tests:
   - Single schema with one table, two columns -> returns 1 TableMetadata with 2 ColumnMetadata
   - Verify TableMetadata.schema_name and table_name match query results
   - Verify ColumnMetadata fields (name, sql_type, is_nullable, ordinal_position, max_length, numeric_precision, numeric_scale) map correctly from query row

2. Multi-table discovery:
   - Two tables in same schema -> returns 2 TableMetadata objects
   - Tables across two schemas -> returns all tables from both

3. View exclusion verification:
   - The SQL query must contain "TABLE_TYPE = 'BASE TABLE'" (verify by inspecting the query string passed to cursor.execute)
   - The SQL query must JOIN INFORMATION_SCHEMA.TABLES with INFORMATION_SCHEMA.COLUMNS

4. Schema filtering verification:
   - Query uses parameterized schema list (verify cursor.execute called with schemas as parameters)
   - Only requested schemas are queried

5. Column grouping tests:
   - Multiple columns for same table are grouped into single TableMetadata
   - Columns preserve ordinal order
   - TableMetadata.columns is a tuple (not list)

6. Edge cases:
   - Empty result set (no tables found) -> returns empty tuple
   - Nullable column mapping: "YES" -> True, "NO" -> False
   - NULL max_length/numeric_precision/numeric_scale -> None

7. Return type:
   - Returns tuple[TableMetadata, ...] (immutable, consistent with Phase 2 patterns)

Mock strategy: Create a mock Connection with a mock cursor. Set cursor.description to match INFORMATION_SCHEMA column names. Set cursor.fetchall() to return predefined rows simulating query results.

Example mock row format (matching the SELECT order from research):
```python
# (TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE, IS_NULLABLE, ORDINAL_POSITION, CHARACTER_MAXIMUM_LENGTH, NUMERIC_PRECISION, NUMERIC_SCALE)
("dbo", "DimCustomer", "SK_CustomerID", "bigint", "NO", 1, None, 19, 0)
("dbo", "DimCustomer", "Name", "varchar", "YES", 2, 100, None, None)
```

Run tests -- they MUST fail.
Commit: "test(03-02): add failing tests for schema discovery"

GREEN phase -- Create src/semantic_model_generator/schema/discovery.py:

```python
"""Schema discovery from Fabric warehouse INFORMATION_SCHEMA.

Reads INFORMATION_SCHEMA.TABLES and INFORMATION_SCHEMA.COLUMNS to
discover BASE TABLEs and their column metadata. Views are excluded
per REQ-29.
"""

from collections import defaultdict
from collections.abc import Sequence

import pyodbc

from semantic_model_generator.domain.types import ColumnMetadata, TableMetadata


def discover_tables(
    conn: pyodbc.Connection,
    schemas: Sequence[str],
) -> tuple[TableMetadata, ...]:
    """Discover BASE TABLEs and columns for specified schemas.

    Joins INFORMATION_SCHEMA.TABLES with COLUMNS, filtering to
    TABLE_TYPE = 'BASE TABLE' to exclude views (REQ-29).

    Args:
        conn: Authenticated pyodbc connection to Fabric warehouse
        schemas: Schema names to discover (e.g., ["dbo", "staging"])

    Returns:
        Tuple of TableMetadata, one per discovered table, with columns
        ordered by ORDINAL_POSITION.
    """
    if not schemas:
        return ()

    schema_placeholders = ", ".join("?" for _ in schemas)

    query = f"""
    SELECT
        t.TABLE_SCHEMA,
        t.TABLE_NAME,
        c.COLUMN_NAME,
        c.DATA_TYPE,
        c.IS_NULLABLE,
        c.ORDINAL_POSITION,
        c.CHARACTER_MAXIMUM_LENGTH,
        c.NUMERIC_PRECISION,
        c.NUMERIC_SCALE
    FROM INFORMATION_SCHEMA.TABLES t
    INNER JOIN INFORMATION_SCHEMA.COLUMNS c
        ON t.TABLE_SCHEMA = c.TABLE_SCHEMA
        AND t.TABLE_NAME = c.TABLE_NAME
    WHERE t.TABLE_TYPE = 'BASE TABLE'
        AND t.TABLE_SCHEMA IN ({schema_placeholders})
    ORDER BY t.TABLE_SCHEMA, t.TABLE_NAME, c.ORDINAL_POSITION
    """

    cursor = conn.cursor()
    cursor.execute(query, list(schemas))
    rows = cursor.fetchall()

    # Group rows by (schema, table)
    table_columns: dict[tuple[str, str], list[ColumnMetadata]] = defaultdict(list)

    for row in rows:
        table_key = (row[0], row[1])
        col = ColumnMetadata(
            name=row[2],
            sql_type=row[3],
            is_nullable=row[4] == "YES",
            ordinal_position=row[5],
            max_length=row[6],
            numeric_precision=row[7],
            numeric_scale=row[8],
        )
        table_columns[table_key].append(col)

    return tuple(
        TableMetadata(
            schema_name=schema,
            table_name=table,
            columns=tuple(cols),
        )
        for (schema, table), cols in table_columns.items()
    )
```

Update schema/__init__.py to re-export discover_tables.

Run tests -- they MUST pass. Run `make check` to confirm all green.
Commit: "feat(03-02): implement schema discovery with view filtering"
  </action>
  <verify>
    1. `make test` passes with all discovery tests green
    2. `make check` passes (lint + typecheck + test)
    3. `python -c "from semantic_model_generator.schema.discovery import discover_tables; print('OK')"` succeeds
    4. Verify the SQL query string contains 'BASE TABLE' and joins TABLES with COLUMNS
  </verify>
  <done>
    discover_tables reads INFORMATION_SCHEMA with TABLE_TYPE='BASE TABLE' filter (views excluded per REQ-29). Joins TABLES with COLUMNS and groups columns by table. Returns immutable tuple of TableMetadata. Parameterized schema filtering. Handles empty results, NULL values, nullable mapping ("YES"->True). All tests pass with mocked pyodbc cursor.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `make check` passes with zero errors
2. All connection tests pass (token encoding, connection string, retry)
3. All discovery tests pass (basic, multi-table, view exclusion, schema filtering, edge cases)
4. `mypy src/` passes in strict mode
5. pyodbc, azure-identity, tenacity are installed and importable
6. Connection factory importable: `from semantic_model_generator.schema.connection import create_fabric_connection`
7. Discovery importable: `from semantic_model_generator.schema.discovery import discover_tables`
8. No actual database connection required for tests (all mocked)
</verification>

<success_criteria>
- create_fabric_connection uses DefaultAzureCredential + token encoding + pyodbc.connect with attrs_before
- encode_token_for_odbc produces correct UTF-16-LE bytes with struct.pack length prefix
- Connection string uses ODBC Driver 18, excludes auth params (UID/PWD/Authentication)
- Retry logic: 3 attempts with exponential backoff on OperationalError/InterfaceError
- discover_tables joins INFORMATION_SCHEMA.TABLES + COLUMNS with TABLE_TYPE='BASE TABLE'
- Schema filtering via parameterized IN clause
- Returns tuple[TableMetadata, ...] with correctly populated ColumnMetadata
- All tests written BEFORE implementation (TDD commits prove RED then GREEN)
- make check passes (lint + typecheck + test)
- pyodbc, azure-identity, tenacity added to dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/03-schema-discovery-classification/03-02-SUMMARY.md`
</output>
